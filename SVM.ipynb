{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/'\n",
    "\n",
    "#CSV filenames\n",
    "deformed01 = 'A_DEFORMED1_C0'\n",
    "rx01 = 'A_RX1_C0'\n",
    "\n",
    "df_def = pd.read_csv(data_folder + deformed01 + '.csv', header = None)\n",
    "df_rx = pd.read_csv(data_folder + rx01 + '.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the frequency of the signal is 3.04878048780488 MHz\n",
      "the total duration of the signal is 10.495999999999993 seconds\n",
      "number of samples : 32000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialisation of clock variables\n",
    "clock = 3048780.48780488\n",
    "delay_in_samples = 304878\n",
    "g_T = 1 / clock\n",
    "g_N = len(df_def)\n",
    "\n",
    "print(\"the frequency of the signal is\", clock * 1e-6, \"MHz\")\n",
    "print(\"the total duration of the signal is\", g_N / clock, \"seconds\")\n",
    "print(\"number of samples :\", len(df_def))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Removes the silence at the end of the audio\n",
    "\n",
    "    Args:\n",
    "        df (array[][]): the data from the csv\n",
    "\n",
    "    Returns:\n",
    "        array[][]: the data without the silence at the end\n",
    "    \"\"\"\n",
    "    epsilon =  1e-1\n",
    "    #remove the silence at the end: \n",
    "    for i in range(len(df) - 1, 0, -1):\n",
    "        if(df[0][i] > epsilon):\n",
    "            print(\"deleted\", (len(df) - i) / clock, \"samples from the end because we assume it is silence from\", len(df) / clock, \"samples\")\n",
    "            return df[:i]\n",
    "\n",
    "def split_csv(filename):\n",
    "    \"\"\"Function that divides the data set in samples of 126ms \n",
    "\n",
    "    Args:\n",
    "        filename (csv): The csv containing the data\n",
    "\n",
    "    Returns:\n",
    "        array[][]: the signal split in samples of 126ms\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_folder + filename + '.csv', header = None)\n",
    "    df = clean_data(df)\n",
    "    df.rename(columns = {0:'amplitude'}, inplace = True)\n",
    "    point_per_sample = int(160e-3 * clock)\n",
    "    \n",
    "    nb_of_samples = int(len(df)/point_per_sample)\n",
    "    data = df[:nb_of_samples * point_per_sample]\n",
    "    return np.array_split(data, nb_of_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wavelet_transform(data,widths):  \n",
    "    \"\"\"Computes the Continuous Wavelet Transform of the signal\n",
    "\n",
    "    Args:\n",
    "        data (2 dimensional array): The signal on which we want to perform the Continuous Wavelet Transform\n",
    "        widths ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        (M,) sequence: The width used for the transform\n",
    "    \"\"\"\n",
    "    cwt = []\n",
    "    for i in data:\n",
    "        cwt.append(np.transpose(signal.cwt(i['amplitude'], signal.ricker, widths)))\n",
    "\n",
    "    return cwt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(cwt_def,cwt_rx):\n",
    "    \"\"\" Aggregates the slices of 126ms into one signal\n",
    "\n",
    "    Args:\n",
    "        cwt_def (3-dim array): array we want to flatten to 2-dim\n",
    "        cwt_rx (3-dim array): array we want to flatten to 2-dim\n",
    "    Returns:\n",
    "        (array): 2-dim array\n",
    "        (array): 2-dim array\n",
    "    \"\"\"\n",
    "    deformed = []\n",
    "    for sublist in cwt_def:\n",
    "        for item in sublist:\n",
    "            deformed.append(item)\n",
    "\n",
    "    rx = []\n",
    "    for sublist in cwt_rx:\n",
    "        for item in sublist:\n",
    "            rx.append(item)\n",
    "    return deformed,rx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_test_train(deformed_data,rx_data):\n",
    "    \"\"\"Generates the train and test sets by selecting random portions of the signal with no overlap\n",
    "        at a 70-30% ratio\n",
    "\n",
    "    Args:\n",
    "        train_len (int): length of the train set\n",
    "        test_len (int): length of the test set\n",
    "\n",
    "    Returns:\n",
    "       (array,array): train set and test set and corresponding targets\n",
    "    \"\"\"\n",
    "    # Threshold at which the svm runs in an acceptable time\n",
    "    max_tresh = 70000\n",
    "    train_len= max_tresh//2\n",
    "    test_len = int(((train_len*.3)/.7)//2)\n",
    "    rand_train = random.randrange(0,len(rx))\n",
    "    rand_test =  random.randrange(0,len(rx))\n",
    "    # The test and train set do not overlap\n",
    "    while (rand_test in range(rand_train, rand_train + train_len)):\n",
    "         rand_test =  random.randrange(0,len(rx), train_len)\n",
    "    return  np.concatenate((deformed_data[rand_train:rand_train + train_len],rx_data[rand_train:rand_train + train_len])) ,np.concatenate((deformed_data[rand_test:rand_test+test_len],rx_data[rand_test:rand_test+test_len])), np.concatenate((np.ones(train_len),np.zeros(train_len))),np.concatenate((np.ones(test_len),np.zeros(test_len)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data before feeding it to the PCA algorithm\n",
    "def standardize(train_set, test_set):\n",
    "    \"\"\"Standarize the data before feeding it to the PCA algorithm\n",
    "\n",
    "    Args:\n",
    "        train_set (array like (n-samples, n_features): The train set\n",
    "        test_set (array like (n-samples, n_features)): The test set\n",
    "\n",
    "    Returns:\n",
    "       (array (n-samples, n_features)): the standardized train set  \n",
    "       (array (n-samples, n_features)): the standardized test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only\n",
    "    scaler.fit(train_set)\n",
    "    # Apply transform to both the training set and the test set\n",
    "    return scaler.transform(train_set), scaler.transform(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(train,test):\n",
    "    \"\"\"PCA for feature extraction on the standardized data\n",
    "\n",
    "    Args:\n",
    "        train (array (n-samples, n_features)): train set we want to extract features from\n",
    "        test (array (n-samples, n_features)): test set we want to extract features from\n",
    "\n",
    "    Returns:\n",
    "        (array (n-samples, n_features)): dimension-reduced train set \n",
    "        (array (n-samples, n_features)): dimension-reduced test-set \n",
    "    \"\"\"\n",
    "    #We chose the minimal number of prinicpal component such that 95% of the variance is retained\n",
    "    pca = PCA(.95)\n",
    "    #fit the data onto the vectors computed by the algorithm\n",
    "    pca.fit(train)\n",
    "    print(pca.n_components_ ,\"components were used to capture the data instead of\", train.shape[1])\n",
    "    #does the projection\n",
    "    return pca.transform(train),pca.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runSVM(train,test,target,kernel):\n",
    "    \"\"\"SVM \n",
    "\n",
    "    Args:\n",
    "        train ((array (n-samples, n_features)): train set\n",
    "        test ((array (n-samples, n_features)): test set\n",
    "        target ((array (n-samples,)): target\n",
    "        kernel (string): kernel to use for the SVM\n",
    "\n",
    "    Returns:\n",
    "        (array (n-samples,): prediction given by the SVM\n",
    "    \"\"\"\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel=kernel)\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(train, target)\n",
    "    #Predict the response for test dataset\n",
    "    return clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getStatistics(y_test, y_pred):\n",
    "    \"\"\"Function to compute accuracy,recall,f1-score and more\n",
    "\n",
    "    Args:\n",
    "        y_test ((array (n-samples,)): test set target\n",
    "        y_pred ((array (n-samples,)): classification given by the model\n",
    "    \"\"\"\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "    # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Confusion matrix\", confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted 1.3556525359999991 samples from the end because we assume it is silence from 10.495999999999993 samples\n",
      "deleted 1.2826049679999991 samples from the end because we assume it is silence from 10.495999999999993 samples\n",
      "4 components were used to capture the data instead of 29\n",
      "Accuracy: 0.5633333333333334\n",
      "Precision: 0.5616242864556306\n",
      "Recall: 0.5772\n",
      "Confusion matrix [[4121 3379]\n",
      " [3171 4329]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.55      0.56      7500\n",
      "         1.0       0.56      0.58      0.57      7500\n",
      "\n",
      "    accuracy                           0.56     15000\n",
      "   macro avg       0.56      0.56      0.56     15000\n",
      "weighted avg       0.56      0.56      0.56     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the data split in 126 ms slices\n",
    "data_deformed = split_csv(deformed01)\n",
    "data_rx = split_csv(rx01)\n",
    "#Compute the wavelet transform of each slice\n",
    "cwt_def = wavelet_transform(data_deformed,np.arange(1,30))\n",
    "cwt_rx = wavelet_transform(data_rx,np.arange(1,30))\n",
    "#Agregate the slices into one signal\n",
    "deformed, rx = flatten_data(cwt_def,cwt_rx)\n",
    "#Compute train and test set\n",
    "X_train, X_test,y_train,y_test = generate_test_train(deformed,rx)\n",
    "#Standardize the data\n",
    "stand_train,stand_test = standardize(X_train,X_test)\n",
    "# Perform dimensionality reduction on the data\n",
    "pca_train,pca_test= feature_extraction(stand_test,stand_train)\n",
    "#Run the SVM\n",
    "prediction = runSVM(stand_train,stand_test,y_train,'rbf')\n",
    "getStatistics(y_test,prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the SVM with the neural net features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9903846153846154\n",
      "Precision: 0.9807692307692307\n",
      "Recall: 1.0\n",
      "Confusion matrix [[52  1]\n",
      " [ 0 51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99        53\n",
      "         1.0       0.98      1.00      0.99        51\n",
      "\n",
      "    accuracy                           0.99       104\n",
      "   macro avg       0.99      0.99      0.99       104\n",
      "weighted avg       0.99      0.99      0.99       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_Librosa_features(filename):\n",
    "    \"\"\"Gets the data and features generated by Librosa, a python package for music and audio analysis and standardizes it\n",
    "       The features have been generated in the SequentialNN.ipynb file\n",
    "    Args:\n",
    "        filename (string): file name where the data is contained\n",
    "\n",
    "    Returns:\n",
    "        array: standardized data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename)\n",
    "    data = data.drop(['filename'],axis=1)\n",
    "    labels = data.iloc[:, -1]\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)),encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "def train_test_set(data):\n",
    "    \"\"\"Splits the data into 70-30% train-test sets\n",
    "\n",
    "    Args:\n",
    "        data (array (n-samples, n_features)): data with the features from\n",
    "\n",
    "    Returns:\n",
    "        [array]: the train and test set and thei corresponding targets\n",
    "    \"\"\"\n",
    "    train_len = int((len(data)//2)*0.7)\n",
    "    test_len = (len(data) //2 )- train_len\n",
    "    mid = len(data)//2\n",
    "   \n",
    "\n",
    "    X_train = np.concatenate((data[:train_len],data[mid:mid+train_len]))\n",
    "    X_test = np.concatenate((data[train_len:mid],data[mid+train_len:]))\n",
    "    y_train=np.concatenate((np.ones(train_len),np.zeros(train_len)))\n",
    "    y_test=np.concatenate((np.ones(test_len),np.zeros(test_len)))\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "X,y = get_Librosa_features(\"data.csv\")\n",
    "x_train,x_test,y_train,y_test= train_test_set(X)\n",
    "pred_nn = runSVM(x_train,x_test,y_train,\"rbf\")\n",
    "getStatistics(pred_nn,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dd2691ec88a8b563fe6b792cd5b35b2471a11a89b348b9101d9c34f3b232cce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
